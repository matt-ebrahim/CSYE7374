{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62917a6a",
   "metadata": {},
   "source": [
    "# MedGemmaÂ 4BÂ Multimodal Demo  \n",
    "**Course:**â€¯AI in Healthcare â€” Multimodalâ€¯Medicalâ€¯Chatbots  \n",
    "\n",
    "This notebook shows how to load Googleâ€™s **MedGemma**â€¯4B model from the Huggingâ€¯Face Hub and run multimodal inference on medical images as well as textâ€‘only prompts.\n",
    "\n",
    "> **Educational use only â€“ not for clinical decisionâ€‘making.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262cf0b",
   "metadata": {},
   "source": [
    "CopyrightÂ 2025Â GoogleÂ LLC â€” Licensed under the ApacheÂ 2.0 License.  \n",
    "See <https://www.apache.org/licenses/LICENSE-2.0> for details.\n",
    "\n",
    "The authors make **no warranty** about the model outputs. Always verify conclusions with qualified healthcare professionals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core libraries (takes ~1â€¯min)\n",
    "!pip install --upgrade --quiet accelerate bitsandbytes transformers pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd73dda",
   "metadata": {},
   "source": [
    "## 1Â Â·Â Authenticate with HuggingÂ Face  \n",
    "\n",
    "1. [Create a free Huggingâ€¯Face account](https://huggingface.co).\n",
    "2. Accept the *MedGemma* usage terms on its [model page](https://huggingface.co/google/medgemma-4b-it).\n",
    "3. Generate a **read** access token (`Settingsâ€¯â†’â€¯AccessÂ Tokens`).  \n",
    "\n",
    "### Colab users  \n",
    "*Open the leftâ€‘side **ğŸ”‘Â Secrets** panel â†’â€¯create secret **HF_TOKEN** with your token value.*\n",
    "\n",
    "### Local Jupyter users  \n",
    "The cell below will prompt you for the token on first run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba72744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "google_colab = \"google.colab\" in sys.modules and not os.environ.get(\"VERTEX_PRODUCT\")\n",
    "\n",
    "if google_colab:\n",
    "    # Colab: retrieve stored secret\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "else:\n",
    "    # Local / ColabÂ Enterprise: interactive login if needed\n",
    "    from huggingface_hub import get_token, notebook_login\n",
    "    if get_token() is None:\n",
    "        notebook_login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b1912a",
   "metadata": {},
   "source": [
    "## 2Â Â·Â Load the MedGemmaÂ 4B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eafd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_variant = \"4b-it\"\n",
    "model_id = f\"google/medgemma-{model_variant}\"\n",
    "\n",
    "model_kwargs = dict(\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True),  # fits on T4â€¯GPU\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"image-text-to-text\", model=model_id, model_kwargs=model_kwargs)\n",
    "pipe.model.generation_config.do_sample = False\n",
    "\n",
    "print(\"Model loaded on\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6564ada2",
   "metadata": {},
   "source": [
    "## 3Â Â·Â ExampleÂ 1 â€” Describe a chestÂ Xâ€‘ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f89a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests, os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Download sample image\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\n",
    "image_path = \"sample_xray.png\"\n",
    "if not os.path.exists(image_path):\n",
    "    Image.open(requests.get(image_url, stream=True).raw).save(image_path)\n",
    "\n",
    "prompt = \"Describe this chest Xâ€‘ray\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are an expert radiologist.\"}]},\n",
    "    {\"role\": \"user\",   \"content\": [{\"type\": \"text\", \"text\": prompt},\n",
    "                                   {\"type\": \"image\", \"image\": Image.open(image_path)}]}\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=300)\n",
    "response = output[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "display(Markdown(f\"**Prompt:** {prompt}\"))\n",
    "display(Image.open(image_path))\n",
    "display(Markdown(f\"**MedGemma:** {response}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c5164",
   "metadata": {},
   "source": [
    "## 4Â Â·Â ExampleÂ 2 â€” Textâ€‘only medical question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"List key differences between bacterial and viral pneumonia.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful medical assistant.\"}]},\n",
    "    {\"role\": \"user\",   \"content\": [{\"type\": \"text\", \"text\": prompt}]}\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=500)\n",
    "response = output[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(f\"**MedGemma:**\\n\\n{response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e49b5",
   "metadata": {},
   "source": [
    "## 5Â Â·Â Your Turn  \n",
    "\n",
    "1. Replace `image_url` with a pathology or dermatology image and ask for findings.  \n",
    "2. Try different prompts such as *â€œSuggest next diagnostic stepsâ€* or *â€œSummarize key abnormal findings.â€*  \n",
    "3. Adjust `max_new_tokens` to control answer length.  \n",
    "\n",
    "Feel free to experiment â€” and remember: **never rely solely on model output for clinical decisions**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

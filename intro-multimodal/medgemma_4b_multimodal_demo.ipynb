{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62917a6a",
   "metadata": {},
   "source": [
    "# MedGemma 4B Multimodal Demo  \n",
    "**Course:** AI in Healthcare — Multimodal Medical Chatbots  \n",
    "\n",
    "This notebook shows how to load Google’s **MedGemma** 4B model from the Hugging Face Hub and run multimodal inference on medical images as well as text‑only prompts.\n",
    "\n",
    "> **Educational use only – not for clinical decision‑making.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262cf0b",
   "metadata": {},
   "source": [
    "Copyright 2025 Google LLC — Licensed under the Apache 2.0 License.  \n",
    "See <https://www.apache.org/licenses/LICENSE-2.0> for details.\n",
    "\n",
    "The authors make **no warranty** about the model outputs. Always verify conclusions with qualified healthcare professionals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core libraries (takes ~1 min)\n",
    "!pip install --upgrade --quiet accelerate bitsandbytes transformers pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd73dda",
   "metadata": {},
   "source": [
    "## 1 · Authenticate with Hugging Face  \n",
    "\n",
    "1. [Create a free Hugging Face account](https://huggingface.co).\n",
    "2. Accept the *MedGemma* usage terms on its [model page](https://huggingface.co/google/medgemma-4b-it).\n",
    "3. Generate a **read** access token (`Settings → Access Tokens`).  \n",
    "\n",
    "### Colab users  \n",
    "*Open the left‑side **🔑 Secrets** panel → create secret **HF_TOKEN** with your token value.*\n",
    "\n",
    "### Local Jupyter users  \n",
    "The cell below will prompt you for the token on first run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba72744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "google_colab = \"google.colab\" in sys.modules and not os.environ.get(\"VERTEX_PRODUCT\")\n",
    "\n",
    "if google_colab:\n",
    "    # Colab: retrieve stored secret\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "else:\n",
    "    # Local / Colab Enterprise: interactive login if needed\n",
    "    from huggingface_hub import get_token, notebook_login\n",
    "    if get_token() is None:\n",
    "        notebook_login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b1912a",
   "metadata": {},
   "source": [
    "## 2 · Load the MedGemma 4B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eafd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_variant = \"4b-it\"\n",
    "model_id = f\"google/medgemma-{model_variant}\"\n",
    "\n",
    "model_kwargs = dict(\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True),  # fits on T4 GPU\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"image-text-to-text\", model=model_id, model_kwargs=model_kwargs)\n",
    "pipe.model.generation_config.do_sample = False\n",
    "\n",
    "print(\"Model loaded on\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6564ada2",
   "metadata": {},
   "source": [
    "## 3 · Example 1 — Describe a chest X‑ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f89a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests, os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Download sample image\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\n",
    "image_path = \"sample_xray.png\"\n",
    "if not os.path.exists(image_path):\n",
    "    Image.open(requests.get(image_url, stream=True).raw).save(image_path)\n",
    "\n",
    "prompt = \"Describe this chest X‑ray\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are an expert radiologist.\"}]},\n",
    "    {\"role\": \"user\",   \"content\": [{\"type\": \"text\", \"text\": prompt},\n",
    "                                   {\"type\": \"image\", \"image\": Image.open(image_path)}]}\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=300)\n",
    "response = output[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "display(Markdown(f\"**Prompt:** {prompt}\"))\n",
    "display(Image.open(image_path))\n",
    "display(Markdown(f\"**MedGemma:** {response}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c5164",
   "metadata": {},
   "source": [
    "## 4 · Example 2 — Text‑only medical question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"List key differences between bacterial and viral pneumonia.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful medical assistant.\"}]},\n",
    "    {\"role\": \"user\",   \"content\": [{\"type\": \"text\", \"text\": prompt}]}\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=500)\n",
    "response = output[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(f\"**MedGemma:**\\n\\n{response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e49b5",
   "metadata": {},
   "source": [
    "## 5 · Your Turn  \n",
    "\n",
    "1. Replace `image_url` with a pathology or dermatology image and ask for findings.  \n",
    "2. Try different prompts such as *“Suggest next diagnostic steps”* or *“Summarize key abnormal findings.”*  \n",
    "3. Adjust `max_new_tokens` to control answer length.  \n",
    "\n",
    "Feel free to experiment — and remember: **never rely solely on model output for clinical decisions**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
